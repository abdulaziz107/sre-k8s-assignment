#!/bin/bash

# Comprehensive failure simulation demo script
# This script demonstrates various failure scenarios and recovery mechanisms
set -e

echo "ğŸ¬ SRE Assignment - Failure Simulation Demo"
echo "=========================================="
echo ""

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m' # No Color

# Function to print section headers
print_section() {
    echo ""
    echo -e "${BLUE}ğŸ” $1${NC}"
    echo "=================================="
}

# Function to print status
print_status() {
    local status=$1
    local message=$2
    if [ "$status" = "SUCCESS" ]; then
        echo -e "${GREEN}âœ… $message${NC}"
    elif [ "$status" = "FAILURE" ]; then
        echo -e "${RED}âŒ $message${NC}"
    else
        echo -e "${YELLOW}âš ï¸  $message${NC}"
    fi
}

# Function to wait and show status
wait_and_show() {
    local service=$1
    local namespace=$2
    local timeout=${3:-30}
    
    echo "â³ Waiting $timeout seconds for $service to recover..."
    sleep $timeout
    
    echo "ğŸ“Š Current status of $service:"
    kubectl get pods -n $namespace -l app=$service -o wide
    echo ""
}

print_section "Initial System State"
echo "Current cluster status:"
kubectl get pods -A
echo ""

print_section "Scenario 1: Database Crash Simulation"
echo "ğŸ¯ Objective: Test system resilience when PostgreSQL database crashes"
echo "ğŸ“‹ Expected Behavior:"
echo "   - Database pod should be rescheduled automatically"
echo "   - Services should show temporary connection errors"
echo "   - System should recover once database is back"
echo ""

echo "ğŸ“Š Before crash - Database status:"
kubectl get pods -n infrastructure -l app=postgresql
echo ""

echo "ğŸ’¥ Simulating database crash..."
kubectl delete pod -l app=postgresql -n infrastructure --force --grace-period=0
print_status "SUCCESS" "Database pod deleted"

wait_and_show "postgresql" "infrastructure" 45

echo "ğŸ“ˆ Database recovery verification:"
kubectl logs -n infrastructure deployment/postgresql --tail=10
echo ""

print_section "Scenario 2: Auth Service Crash Simulation"
echo "ğŸ¯ Objective: Test service-level failure and recovery"
echo "ğŸ“‹ Expected Behavior:"
echo "   - Auth service pods should be rescheduled"
echo "   - API service should show connection errors to auth"
echo "   - System should recover once auth service is back"
echo ""

echo "ğŸ“Š Before crash - Auth service status:"
kubectl get pods -n auth -l app=auth-service
echo ""

echo "ğŸ’¥ Simulating auth service crash..."
kubectl delete pod -l app=auth-service -n auth --force --grace-period=0
print_status "SUCCESS" "Auth service pods deleted"

wait_and_show "auth-service" "auth" 30

echo "ğŸ“ˆ Auth service recovery verification:"
kubectl logs -n auth deployment/auth-service --tail=10
echo ""

print_section "Scenario 3: High Traffic Simulation"
echo "ğŸ¯ Objective: Test HPA (Horizontal Pod Autoscaler) functionality"
echo "ğŸ“‹ Expected Behavior:"
echo "   - Image service should scale up due to increased load"
echo "   - HPA should create additional replicas"
echo "   - System should handle increased traffic"
echo ""

echo "ğŸ“Š Before scaling - Image service status:"
kubectl get pods -n image-storage -l app=image-storage-service
echo ""

echo "ğŸ“ˆ Current HPA status:"
kubectl get hpa -n image-storage
echo ""

echo "ğŸš€ Scaling up image service to simulate high traffic..."
kubectl scale deployment image-storage-service -n image-storage --replicas=5
print_status "SUCCESS" "Image service scaled to 5 replicas"

wait_and_show "image-storage-service" "image-storage" 20

echo "ğŸ“Š After scaling - Image service status:"
kubectl get pods -n image-storage -l app=image-storage-service
echo ""

print_section "Scenario 4: Node Failure Simulation"
echo "ğŸ¯ Objective: Test cluster resilience to node failures"
echo "ğŸ“‹ Expected Behavior:"
echo "   - Pods should be rescheduled to healthy nodes"
echo "   - Services should remain available"
echo "   - System should maintain functionality"
echo ""

echo "ğŸ“Š Current node status:"
kubectl get nodes
echo ""

NODE=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')
echo "ğŸ¯ Target node for simulation: $NODE"
echo ""

echo "ğŸ”’ Cordoning node to simulate failure..."
kubectl cordon $NODE
print_status "SUCCESS" "Node $NODE cordoned"

echo "ğŸ”„ Draining node..."
kubectl drain $NODE --ignore-daemonsets --delete-emptydir-data --force
print_status "SUCCESS" "Node $NODE drained"

echo "ğŸ“Š Pod rescheduling status:"
kubectl get pods -A -o wide | grep -v $NODE
echo ""

echo "ğŸ”“ Uncordoning node..."
kubectl uncordon $NODE
print_status "SUCCESS" "Node $NODE uncordoned"

print_section "Scenario 5: Resource Exhaustion Simulation"
echo "ğŸ¯ Objective: Test system behavior under resource pressure"
echo "ğŸ“‹ Expected Behavior:"
echo "   - API service should show high CPU usage"
echo "   - HPA might scale up the service"
echo "   - System should handle resource pressure"
echo ""

echo "ğŸ“Š Before stress test - API service status:"
kubectl get pods -n api -l app=api-service
echo ""

echo "ğŸ’ª Simulating high CPU usage on API service..."
kubectl exec -n api deployment/api-service -- sh -c "while true; do : ; done" &
STRESS_PID=$!
print_status "SUCCESS" "CPU stress test started"

echo "â³ Running stress test for 30 seconds..."
sleep 30

echo "ğŸ›‘ Stopping stress test..."
kill $STRESS_PID 2>/dev/null || true
print_status "SUCCESS" "CPU stress test stopped"

wait_and_show "api-service" "api" 20

print_section "Monitoring & Alerting Verification"
echo "ğŸ“Š Checking for active alerts..."
kubectl port-forward svc/monitoring-kube-prometheus-prometheus 9090:9090 -n monitoring &
PROM_PID=$!
sleep 5

echo "ğŸ” Prometheus alerts status:"
curl -s http://localhost:9090/api/v1/alerts | jq '.data.alerts[] | {name: .labels.alertname, status: .state, severity: .labels.severity}' 2>/dev/null || echo "No active alerts found"

kill $PROM_PID 2>/dev/null || true

print_section "Final System State"
echo "ğŸ“Š Final cluster status:"
kubectl get pods -A
echo ""

echo "ğŸ“ˆ Final HPA status:"
kubectl get hpa -A
echo ""

echo "ğŸ” Recent events:"
kubectl get events --sort-by='.lastTimestamp' | tail -10
echo ""

print_section "Demo Summary"
echo "âœ… All failure scenarios completed successfully!"
echo ""
echo "ğŸ“‹ Key Observations:"
echo "   - Kubernetes automatically rescheduled failed pods"
echo "   - HPA scaled services based on load"
echo "   - Network policies maintained security isolation"
echo "   - Monitoring detected and reported issues"
echo "   - System maintained availability during failures"
echo ""
echo "ğŸ¯ SRE Best Practices Demonstrated:"
echo "   - High Availability through multi-replica deployments"
echo "   - Auto-scaling based on resource usage"
echo "   - Automatic failure recovery"
echo "   - Comprehensive monitoring and alerting"
echo "   - Security through network policies"
echo ""
echo "ğŸ“¹ This demo can be recorded as a video showing:"
echo "   - Real-time pod status changes"
echo "   - HPA scaling in action"
echo "   - Alert generation and notification"
echo "   - System recovery mechanisms"
echo ""
echo -e "${GREEN}ğŸ‰ Failure simulation demo completed successfully!${NC}" 